= Upgrade component version v1 to v2

When migrating from component version v1 to v2, the following breaking changes require manual setup of zone aware replication on existing instances:

* https://github.com/grafana/mimir/blob/main/operations/helm/charts/mimir-distributed/CHANGELOG.md#400[mimir-distributed] enables zone aware replication for ingesters by default. An upgrade without manual migration will incur data loss.


== Prerequisites

You need to be able to execute the following CLI tools locally:

* `yq` https://github.com/mikefarah/yq[yq YAML processor] (Version 4 or newer)

== Setup environment

. Access to API
+
[source,bash]
----
# For example: https://api.syn.vshn.net
# IMPORTANT: do NOT add a trailing `/`. Commands below will fail.
export COMMODORE_API_URL=<lieutenant-api-endpoint>

export CLUSTER_ID=<lieutenant-cluster-id> # Looks like: c-<something>
export TENANT_ID=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${CLUSTER_ID} | jq -r .tenant)

# Parameter key under which your mimir component instance is configured in the tenant repo
export MIMIR_PARAM_KEY=<mimir_param_key>
----

== Upgrade the component without zone aware replication

. Compile the cluster catalog to create a local working directory
+
[source,bash]
----
commodore catalog compile "${CLUSTER_ID}"
----

. Update the component version to v2.x in your cluster configuration:
+
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.components.mimir.version = 2.0.0" ${CLUSTER_ID}.yml

cd ../../..
----

. Add temporary migration configuration:
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.alertmanager.zoneAwareReplication.enabled = false" ${CLUSTER_ID}.yml

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.ingester.zoneAwareReplication.enabled = false" ${CLUSTER_ID}.yml

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.store_gateway.zoneAwareReplication.enabled = false" ${CLUSTER_ID}.yml

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.rollout_operator.enabled = false" ${CLUSTER_ID}.yml

cd ../../..
----
+
This configuration disables zone aware replication. In the first step, the new component version is deployed with this setting disabled.

. Compile and push the cluster catalog
. Wait for all relevant pods to start and become ready

== Migrate Alertmanager

. Update the confiugration for alertmanager
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.alertmanager.zoneAwareReplication.enabled = true" ${CLUSTER_ID}.yml

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.alertmanager.zoneAwareReplication.migration.enabled = true" ${CLUSTER_ID}.yml

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.rollout_operator.enabled = true" ${CLUSTER_ID}.yml

cd ../../..
----

. Compile and push the cluster catalog
. Wait for all alertmanager pods to restart and become ready
. Once that's done, enable the write path for the new zone aware alertmanager:
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.alertmanager.zoneAwareReplication.migration.writePath = true" ${CLUSTER_ID}.yml

cd ../../..
----

. Compile and push the cluster catalog
. Wait for the new zone-aware alertmanager pods to become ready

. Next, set the final configuration (meaning the default configuration) by removing the custom `alertmanager` config block:
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.alertmanager)" ${CLUSTER_ID}.yml

cd ../../..
----
. Compile and push the cluster catalog
. Wait for the old zone-unaware alertmanager pods to be terminated

== Migrate store-gateway

. Scale down the store-gateways
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.store_gateway.replicas = 0" ${CLUSTER_ID}.yml

cd ../../..
----

. Compile and push the cluster catalog
. Wait for all store-gateway pods to terminate

. Next, set the final configuration (meaning the default configuration) by removing the custom `store_gateway` config block:
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.store_gateway)" ${CLUSTER_ID}.yml

cd ../../..
----
. Compile and push the cluster catalog
. Wait for the new store-gwateways to become ready

== Migrate ingesters

. Configure the ingesters to flush data on shutdown
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.mimir.structuredConfig.blocks_storage.tsdb.flush_blocks_on_shutdown = true" ${CLUSTER_ID}.yml
yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.mimir.structuredConfig.ingester.ring.unregister_on_shutdown = true" ${CLUSTER_ID}.yml

cd ../../..
----

. Compile and push the cluster catalog
. Wait for all ingester pods to restart and become ready

. Next, scale down traffic to the ingesters
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.nginx.replicas = 0" ${CLUSTER_ID}.yml
yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.gateway.replicas = 0" ${CLUSTER_ID}.yml

cd ../../..
----

. Compile and push the cluster catalog
. Wait for all nginx and gateway pods to terminate

. Next, scale down the old zone-unaware ingesters as well:
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.ingester.replicas = 0" ${CLUSTER_ID}.yml

cd ../../..
----
. Compile and push the cluster catalog
. Wait for the ingesters to terminate

. Now, enable the zone-aware ingesters
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i ".parameters.${MIMIR_PARAM_KEY}.helm_values.ingester.zoneAwareReplication.enabled = true" ${CLUSTER_ID}.yml
yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.ingester.replicas)" ${CLUSTER_ID}.yml
yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.mimir)" ${CLUSTER_ID}.yml

cd ../../..
----
. Compile and push the cluster catalog
. Wait for the new ingesters to become ready

. Next, set the final configuration (meaning the default configuration) by removing the custom configuration:
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.ingester)" ${CLUSTER_ID}.yml
yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.nginx)" ${CLUSTER_ID}.yml
yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.gateway)" ${CLUSTER_ID}.yml
yq eval -i "del(.parameters.${MIMIR_PARAM_KEY}.helm_values.rollout_operator)" ${CLUSTER_ID}.yml

cd ../../..
----
+
Alternatively, if your cluster configuration didn't previously contain parameters for the mimir component, you can remove the entire parameter block for your component instance.
+
[source,bash]
----
cd inventory/classes/$TENANT_ID

yq eval -i "del(.parameters.${MIMIR_PARAM_KEY})" ${CLUSTER_ID}.yml

cd ../../..
----

. Compile and push the cluster catalog
. Wait for all relevant pods to become ready

== Cleanup

. If all your mimir instances are migrated, move the component version parameter to the appropriate place in your hierarchy
