apiVersion: v1
data:
  nginx.conf: "worker_processes  5;  ## Default: 1\nerror_log  /dev/stderr;\npid \
    \       /tmp/nginx.pid;\nworker_rlimit_nofile 8192;\n\nevents {\n  worker_connections\
    \  4096;  ## Default: 1024\n}\n\nhttp {\n  client_body_temp_path /tmp/client_temp;\n\
    \  proxy_temp_path       /tmp/proxy_temp_path;\n  fastcgi_temp_path     /tmp/fastcgi_temp;\n\
    \  uwsgi_temp_path       /tmp/uwsgi_temp;\n  scgi_temp_path        /tmp/scgi_temp;\n\
    \n  default_type application/octet-stream;\n  log_format   main '$remote_addr\
    \ - $remote_user [$time_local]  $status '\n        '\"$request\" $body_bytes_sent\
    \ \"$http_referer\" '\n        '\"$http_user_agent\" \"$http_x_forwarded_for\"\
    ';\n  access_log   /dev/stderr  main;\n\n  sendfile     on;\n  tcp_nopush   on;\n\
    \  resolver kube-dns.kube-system.svc.cluster.local;\n\n  # Ensure that X-Scope-OrgID\
    \ is always present, default to the no_auth_tenant for backwards compatibility\
    \ when multi-tenancy was turned off.\n  map $http_x_scope_orgid $ensured_x_scope_orgid\
    \ {\n    default $http_x_scope_orgid;\n    \"\" \"anonymous\";\n  }\n\n  server\
    \ {\n    listen 8080;\n\n    location = / {\n      return 200 'OK';\n      auth_basic\
    \ off;\n    }\n\n    proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\n\n\
    \    # Distributor endpoints\n    location /distributor {\n      proxy_pass  \
    \    http://mimir-distributor-headless.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n    location = /api/v1/push {\n      proxy_pass      http://mimir-distributor-headless.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n\n    # Alertmanager endpoints\n    location /alertmanager {\n      proxy_pass\
    \      http://mimir-alertmanager.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n    location = /multitenant_alertmanager/status {\n      proxy_pass  \
    \    http://mimir-alertmanager.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n    location = /api/v1/alerts {\n      proxy_pass      http://mimir-alertmanager.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n\n    # Ruler endpoints\n    location /prometheus/config/v1/rules {\n\
    \      proxy_pass      http://mimir-ruler.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n    location /prometheus/api/v1/rules {\n      proxy_pass      http://mimir-ruler.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n\n    location /prometheus/api/v1/alerts {\n      proxy_pass      http://mimir-ruler.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n    location = /ruler/ring {\n      proxy_pass      http://mimir-ruler.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n\n    # Rest of /prometheus goes to the query frontend\n    location /prometheus\
    \ {\n      proxy_pass      http://mimir-query-frontend.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n\n    # Buildinfo endpoint can go to any component\n    location = /api/v1/status/buildinfo\
    \ {\n      proxy_pass      http://mimir-query-frontend.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n\n    # Compactor endpoint for uploading blocks\n    location /api/v1/upload/block/\
    \ {\n      proxy_pass      http://mimir-compactor.syn-mimir.svc.cluster.local:8080$request_uri;\n\
    \    }\n  }\n}\n"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: nginx
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.4.0
    helm.sh/chart: mimir-distributed-3.3.0
  name: mimir-nginx
  namespace: syn-mimir
